import [defs, re]
Tokens = defs.Tokens

isalpha = >(c, first) {
  if (first) {
    return bool(re.match("^[A-Za-z_]+$", c))
  } else {
    return bool(re.match("^[A-Za-z0-9_]+$", c))
  }
}
isint = >(c) { return bool(re.match("^[0-9]+$", c)) }
isnone = >(c) { return bool(re.match("^\s*$", c)) }
_tok = >(v, c, pos) { return [v, c, pos] }


fn tokenize(text) {
  out = []
  optable = {
    "(": Tokens:POpen, ")": Tokens:PClose, "{": Tokens:BOpen, "}": Tokens:BClose,  "[": Tokens:SOpen, "]": Tokens:SClose,
    "+": Tokens:BinOp, "-": Tokens:BinOp,  "*": Tokens:BinOp, "%": Tokens:BinOp,   "<": Tokens:Smaller, ">": Tokens:Bigger, "in": Tokens:BinOp,
    ".": Tokens:Dot,   ",": Tokens:Comma,  ":": Tokens:Colon, ";": Tokens:Semi,    "&": Tokens:And,     "|": Tokens:Or,
  }
  keywords = {
    "import": Tokens:Import,
    "fn": Tokens:Fn, "return": Tokens:Ret,
    "if": Tokens:If,  "else": Tokens:Else, "for": Tokens:For, "foreach": Tokens:Foreach,
    "while": Tokens:While,  "class": Tokens:Class,
    "async": Tokens:Async,  "try": Tokens:Try, "catch": Tokens:Catch,
  }

  for (i=0; i<len(text); i=i+1) {
    char = text[i]
    if (char in optable) { out.append(_tok(char, optable[char], i)) }
    else if (char == "/") { // comment or divisor
      if text[i+1] == "/" {
        while i<len(text) & text[i] != "\n" { i=i+1 }
      } else if text[i+1] == "*" {  /* multiline comments */
        i=i+2
        while i<len(text) &  not(text[i] == "*" & text[i+1] == "/") { i=i+1 }
        i=i+1     // advance for the remaining */
      } else { out.append(_tok(char, Tokens["BinOp"], i)) }

    } else if (char == "!") {
      i=i+1   out.append(_tok("!=", Tokens["NEIf"], i))
    } else if (char == "=") {
      if (text[i+1] == "=") { i=i+1  out.append(_tok("==", Tokens["EIf"], i)) }
      else                  { out.append(_tok(char, Tokens["Equals"], i)) }

    } else if (char == "\"") {
      i=i+1  acc = ""
      while i<len(text) { 
        if (text[i] == "\"") { break }
        if (text[i] == "\\") {
          i=i+1
          if (i<len(text)) {
            if (text[i] == "\"") { acc = acc + "\"" }
            else if (text[i] == "\\") { acc = acc + "\\" }
            else if (text[i] == "n") { acc = acc + "\n" }
            else if (text[i] == "t") { acc = acc + "\t" }
            else  { acc = acc + "\\" + text[i] }
          }
        } else { acc = acc + text[i] }
        i=i+1
      }
      out.append(_tok(acc, Tokens["Str"], i))
    
    
    } else if (isint(char)) {
      acc = ""  hasDot = False
      while i<len(text) & (isint(text[i]) | (text[i] == "." & hasDot != True)) {
        if (text[i] == ".") { hasDot = True }
        acc = acc + text[i]
        i=i+1
      }

      out.append(_tok(acc, Tokens["Int"], i))
      i=i-1
    } else if (isalpha(char, True)) {
      acc = ""
      while i<len(text) & isalpha(text[i], False)  { acc = acc + text[i]   i=i+1 }

      tok = Tokens["Word"]
      if (acc in keywords) { tok = keywords[acc] }
      out.append(_tok(acc, tok, i))
      i=i-1
    } else if (isnone(char) != True) {
      raise(Exception("lexer:unrecognized: " + char))
    }
  }

  out.append(_tok("<EOF>", Tokens:EOF, i))
  return out
}