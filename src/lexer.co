import "defs, re"
Tokens = defs.Tokens

isalpha = >(c, first) {
  if (first) {
    return bool(re.match("^[A-Za-z_]+$", c))
  } else {
    return bool(re.match("^[A-Za-z0-9_]+$", c))
  }
}
isint = >(c) { return bool(re.match("^[0-9]+$", c)) }
isnone = >(c) { return bool(re.match("^\s*$" ,c)) }
_tok = >(v, c) { return [v, c] }


fn tokenize(text) {
  out = []
  optable = {
    "(": Tokens["POpen"], ")": Tokens["PClose"], "{": Tokens["BOpen"], "}": Tokens["BClose"],  "[": Tokens["SOpen"], "]": Tokens["SClose"],
    "+": Tokens["BinOp"], "-": Tokens["BinOp"], "*": Tokens["BinOp"], "%": Tokens["BinOp"],   "<": Tokens["Smaller"], ">": Tokens["Bigger"], "in": Tokens["BinOp"],
    ".": Tokens["Dot"],   ",": Tokens["Comma"], ":": Tokens["Colon"], ";": Tokens["Semi"],    "&": Tokens["And"],     "|": Tokens["Or"],
  }
  keywords = {
    "import": Tokens["Import"],
    "fn": Tokens["Fn"], "return": Tokens["Ret"],
    "if": Tokens["If"], "else": Tokens["Else"], "for": Tokens["For"],
    "while": Tokens["While"], "class": Tokens["Class"],
    "async": Tokens["Async"],
  }

  for (i=0; i<len(text); i=i+1) {
    char = text[i]
    if (char in optable) { out.append(_tok(char, optable[char])) }
    else if (char == "/") { // comment or divisor
      if (text[i+1] == "/") {
        for (_=0; i<len(text) & text[i] != "\n"; _=0) { i=i+1 }
      } else { out.append(_tok(char, Tokens["BinOp"])) }

    } else if (char == "!") {
      i=i+1   out.append(_tok("!=", Tokens["NEIf"]))
    } else if (char == "=") {
      if (text[i+1] == "=") { i=i+1  out.append(_tok("==", Tokens["EIf"])) }
      else                  { out.append(_tok(char, Tokens["Equals"])) }

    } else if (char == "\"") {
      i=i+1  acc = ""
      for (_=0; i<len(text); _=0) { 
        if (text[i] == "\"") { break }
        if (text[i] == "\\") {
          i=i+1
          if (i<len(text)) {
            if (text[i] == "\"") { acc = acc + "\"" }
            else if (text[i] == "\\") { acc = acc + "\\" }
            else if (text[i] == "n") { acc = acc + "\n" }
            else if (text[i] == "t") { acc = acc + "\t" }
            else  { acc = acc + "\\" + text[i] }
          }
        } else { acc = acc + text[i] }
        i=i+1
      }
      out.append(_tok(acc, Tokens["Str"]))
    
    
    } else if (isint(char)) {
      acc = ""  hasDot = False
      for (_=0; i<len(text) & (isint(text[i]) | (text[i] == "." & hasDot != True)); _=0) {
        if (text[i] == ".") { hasDot = True }
        acc = acc + text[i]
        i=i+1
      }

      out.append(_tok(acc, Tokens["Int"]))
      i=i-1
    } else if (isalpha(char, True)) {
      acc = ""
      for (_=0; i<len(text) & isalpha(text[i], False); _=0) { acc = acc + text[i]   i=i+1 }

      tok = Tokens["Word"]
      if (acc in keywords) { tok = keywords[acc] }
      out.append(_tok(acc, tok))
      i=i-1
    } else if (isnone(char) != True) {
      raise(Exception("lexer:unrecognized: " + char))
    }
  }

  out.append(_tok("<EOF>", Tokens["EOF"]))
  return out
}